{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2e1a4e-aab4-4c54-b663-b6b7d4a7fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2264a12d-7691-4b92-8026-f2fbd6ba1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=['For the people against it, let dogs rule you then','dogs promote active lifestyle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7b22138e-7c8b-47b5-9cf8-89b11487dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal=\"dogs should be restricted from certain areas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d560b992-2f17-4ae9-bad8-82ac2930b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a61a56-cf94-49a6-8fce-209cde21e558",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b5650d0b-85ed-4ae9-9fa5-531a10a09fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_messages(contents):\n",
    "    formatted_messages = []\n",
    "    for content in contents:\n",
    "        formatted_message = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"Classify user text based on support for the proposal: {proposal}. Use these guidelines:\n",
    "\n",
    "Strongly Agree: Explicit, emphatic support for the proposal.\n",
    "Agree: General support for the proposal. This includes interpreting sarcastic or rhetorical remarks as agreement if they imply that the alternative to the proposal is unreasonable or undesirable.\n",
    "Neutral: Ambiguous or no clear position on the proposal.\n",
    "Disagree: Opposes the proposal, directly or through positive mentions of opposing aspects.\n",
    "Strongly Disagree: Explicit, emphatic opposition to the proposal.\n",
    "When evaluating sarcasm or rhetorical language, focus on the implied stance rather than the literal wording. For example, a sarcastic remark suggesting an extreme alternative to the proposal likely indicates agreement with the proposal.\n",
    "\n",
    "Output format:\n",
    "Answer: \"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (content.strip())\n",
    "            }\n",
    "        ]\n",
    "        formatted_messages.append(formatted_message)\n",
    "    return formatted_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "551ce033-575f-4c9c-8525-8e7dc68cd207",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=convert_messages(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b2995b-68a9-4957-8ac3-3f4e577655db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Filter out comments longer than 3990 tokens\n",
    "filtered_comments = [comment for comment in comments if len(comment) <= 3900]\n",
    "\n",
    "# Now, filtered_comments will contain only the comments with 3900 characters or less\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "78ce82a4-d3cf-47af-9c77-824fa93044fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cost_to_score_comments(model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Estimate the number of tokens in a dataframe's comment_body column\"\"\"\n",
    "    num_tokens = 0\n",
    "    completion=0\n",
    "    filtered_messages=[]\n",
    "    for message in messages:\n",
    "        num = num_tokens_from_messages(message, model)\n",
    "        if num <=3900:\n",
    "            num_tokens+= num\n",
    "            filtered_messages.append(message)\n",
    "            completion +=5\n",
    "    return ((num_tokens * (0.0023 / 1000))+(completion * (0.0031 / 1000) ),filtered_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1942b55c-b0ae-4ce4-ae49-1e91979b8e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n",
      "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n"
     ]
    }
   ],
   "source": [
    "c,m=estimate_cost_to_score_comments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3add62a6-9ecc-4e97-8dce-3df28a8eabd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
